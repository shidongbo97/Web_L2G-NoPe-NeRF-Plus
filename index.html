<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning Neural Volumetric Pose Features for Camera Localization">
  <meta name="keywords" content="Absolute pose regression, Pose feature, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning Neural Volumetric Pose Features for Camera Localization</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Learning Neural Volumetric Pose Features<br> for Camera Localization</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">ECCV 2024</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Jingyu Lin</a><sup>1*&dagger;</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block"><a href="https://gujiaqivadin.github.io/">Jiaqi Gu</a><sup>2&dagger;</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block"><a href="https://scholar.google.com.eg/citations?user=pdx6Lg8AAAAJ">Bojian Wu</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block"><a href="https://lubinfan.github.io/">Lubin Fan</a><sup>2&ddagger;</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Renjie Chen<sup>1&ddagger;</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block">Ligang Liu<sup>1</sup>,&nbsp&nbsp&nbsp</span>
            <span class="author-block"><a href="https://scholar.google.com.eg/citations?user=T9AzhwcAAAAJ">Jieping Ye</a><sup>2</sup>&nbsp&nbsp&nbsp</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>2</sup>Alibaba Cloud &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>3</sup>Zhejiang University &nbsp&nbsp&nbsp</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>This work was done when Jingyu Lin was an intern at Alibaba Cloud.</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>&dagger;</sup>Equal contributions.</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>&ddagger;</sup>Corresponding authors.</span>
          </div>


          <div class="column has-text-centered">
<!--            <div class="publication-links">-->
<!--               PDF Link.-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.12800"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="index.html#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Comming Soon)</span>
                  </a>
              </span>
              <!-- Huggingface Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">&#129303</span>
                  <span>Demo (Comming Soon)</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--          <img src="static/images/teaser.png" alt="Teaser image."/>-->
<!--          <h2 class="subtitle has-text-justified">-->
<!--            Groma is a multimodal large language model with exceptional region understanding and visual grounding capabilities.-->
<!--            It can take user-defined region inputs (boxes) as well as generate long-form responses that are grounded to visual context.-->
<!--          </h2>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a novel neural volumetric pose feature, termed PoseMap, designed to enhance camera localization by encapsulating the information between images and the associated camera poses. Our framework leverages an Absolute Pose Regression (APR) architecture, together with an augmented NeRF module. This integration not only facilitates the generation of novel views to enrich the training dataset but also enables the learning of effective pose features. Additionally, we extend our architecture for self-supervised online alignment, allowing our method to be used and fine-tuned for unlabelled images within a unified framework. Experiments demonstrate that our method achieves 14.28% and 20.51% performance gain on average in indoor and outdoor benchmark scenes, outperforming existing APR methods with state-of-the-art accuracy.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
              <img src="static/images/teaser.jpg" alt="Teaser image."/>
              <h2 class="subtitle has-text-justified">
                Groma is a multimodal large language model with exceptional region understanding and visual grounding capabilities.
                It can take user-defined region inputs (boxes) as well as generate long-form responses that are grounded to visual context.
              </h2>
          </div>
        </div>
    </div> -->
  </div>
</section>


<br>
<br>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <img src="static/images/fig1.png" />
        <figcaption>Figure: Pipeline.</figcaption>
        <div class="content has-text-justified">
          <p>
            Given a set of images and the associated camera poses \( \{(I, p)\} \), our goal is to train a neural network that takes a query image \( I^* \) as input and directly outputs its corresponding camera pose \( p^* \).
            Here the camera pose is represented as a \( 3\times 4 \) matrix which is a combination of translation and rotation with regard to a reference coordinate system.
            The whole pipeline is demonstrated in the figure</a>.
            It contains two main entangled modules: APRNet and NeRF-P.
            Concretely, with an input image \( I \), APRNet leverages separate branches to extract image features \( \mathcal{F}_{image}(I) \) and estimates the camera pose \( \hat{p} \).
            With the given ground-truth pose, NeRF-P subsequently renders a synthetic image \( \hat{I} \), which will be forwarded to the feature extraction branch of APRNet and obtain \( \mathcal{F}_{image}(\hat{I}) \).
        </p>
        <p>
            On the other hand, we propose a novel implicit pose features \( \mathcal{F}_{pose}(\hat{p}) \) called <i>PoseMap</i>, by enhancing the volumetric rendering module with an extra pose feature branch on original NeRF architecture, which will be further used for pose prediction.
            The key idea of this design choice is that NeRF itself is an abstraction of visual and geometric information.
            We propose an autoencoder-style pose branch to leverage NeRF to aggregate global attributes from samples of light rays on PoseMap and a pose decoder of a series of MLP decoders for the distillation of implicit pose features.
            This novel combination allows for a more precise and detailed representation of the camera pose.
            In all, APRNet is optimized, with the supervision of the ground-truth PoseMap \( \mathcal{F}_{pose}(p) \), and by minimizing the discrepancies of image features.
        </p>
        </div>
      </div>
    </div>
  </div>
</section>


<br>
<br>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <img src="static/images/fig2.png" />
        <figcaption>Table: Results on 7-scenes dataset.</figcaption>
        <div class="content has-text-justified">
          <p>
            First, we compare our methods with one-frame APR methods on 7-scenes dataset. The Table shows that our method achieves the best results on all indoor scenes.
            To be specific, our main pipeline, denoted as PMNet, achieves state-of-the-art performance. Moreover, PMNet<sub>ud</sub> finetuned in the self-supervising stage with unlabelled data further enhances the pose regression accuracy. The translation and rotation errors are reduced by 14.28% (0.07 &rarr; 0.06) and 12.67%(2.21 &rarr; 1.93) on average, respectively. 
            For the Heads and Pumpkin scenes, our results are comparable to those of DFNet, though not superior. Upon reviewing the dataset, we found that these scenes are relatively small, with limited visible range and camera pose variations. Consequently, the benefits of our PoseMap may not be particularly significant.
          </p>
        </div>
        <img src="static/images/fig3.png" />
        <figcaption>Table: Results on Cambridge Landmarks dataset.</figcaption>
        <div class="content has-text-justified">
          <p>
            Then we evaluate our methods on the more challenging dataset, i.e., Cambridge Landmarks. As shown in the Table, our PMNet leads to the dominant performance advantage over all scenes in single-frame APR methods. 
            Compared to DFNet<sub>dm</sub> which also uses unlabelled data for online training, our method gets a performance gain by 20.51% (0.39 &rarr; 0.31) and 17.71% (0.96 &rarr; 0.79) for translations and rotations, respectively.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<br>
<br>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualization</h2>
        <img src="static/images/fig4.png" />
        <figcaption>Figure: Visualization.</figcaption>
        <img src="static/images/fig5.png" />
        <figcaption>Figure: 6-DOF Camera Pose Regression.</figcaption>
        <div class="content has-text-justified">
          <p>
            We visualize the camera localization sequences on 7-scenes dataset in Figure. It shows that our trajectories mostly coincide with the ground truth that means our method could estimate camera positions with high accuracy. Comparing with the SOTA method (i.e., DFNet), our method achieves better estimation results since our trajectories are much closer to the ground truth, and the number of high rotation error zones is less.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<br>
<br>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{lin2024posemap,
    author = {Jingyu Lin, Jiaqi Gu, Bojian Wu, Lubin Fan, Renjie Chen, Ligang Liu, Jieping Ye},
    title = {Learning Neural Volumetric Pose Features for Camera Localization},
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    year = {2024}
}</code></pre>
  </div>
</section>



</body>
</html>