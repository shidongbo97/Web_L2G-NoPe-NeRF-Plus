<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior">
  <meta name="keywords" content="NeRF, Pose estimation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NoPe-NeRF++: Local-to-Global Optimization of NeRF with No Pose Prior</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">NoPe-NeRF++: Local-to-Global Optimization of NeRF<br> with No Pose Prior</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">Eurographics 2025</span>
          </div>
          <div class="is-size-5 publication-authors">
              <span class="author-block">Dongbo Shi</a><sup>1*&dagger;</sup>,&nbsp&nbsp&nbsp</span>
              <span class="author-block">Shen Cao</a><sup>2&dagger;</sup>,&nbsp&nbsp&nbsp</span>
              <span class="author-block">Jinhui Guo</a><sup>2&dagger;</sup>,&nbsp&nbsp&nbsp</span>
              <span class="author-block"><a href="https://scholar.google.com.eg/citations?user=pdx6Lg8AAAAJ">Bojian Wu</a><sup>3</sup>,&nbsp&nbsp&nbsp</span>
              <span class="author-block"><a href="https://lubinfan.github.io/">Lubin Fan</a><sup>2&ddagger;</sup>,&nbsp&nbsp&nbsp</span>
              <span class="author-block"><a href="http://staff.ustc.edu.cn/~renjiec/">Renjie Chen</a><sup>1&ddagger;</sup>,&nbsp&nbsp&nbsp</span>
              <span class="author-block"><a href="http://staff.ustc.edu.cn/~lgliu/">Ligang Liu</a><sup>1</sup>,&nbsp&nbsp&nbsp</span>
              <span class="author-block"><a href="https://scholar.google.com.eg/citations?user=T9AzhwcAAAAJ">Jieping Ye</a><sup>2</sup>&nbsp&nbsp&nbsp</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>2</sup>Alibaba Cloud &nbsp&nbsp&nbsp</span>
            <span class="author-block"><sup>3</sup>Zhejiang University &nbsp&nbsp&nbsp</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>This work was done when author was an intern at Alibaba Cloud.</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>&dagger;</sup>Equal contributions.</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>&ddagger;</sup>Corresponding authors.</span>
          </div>


          <div class="column has-text-centered">
<!--            <div class="publication-links">-->
<!--               PDF Link.-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fas fa-file-pdf"></i>-->
<!--                  </span>-->
<!--                  <span>Paper</span>-->
<!--                </a>-->
<!--              </span>-->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.12800"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="index.html#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Comming Soon)</span>
                  </a>
              </span>
              <!-- Huggingface Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">&#129303</span>
                  <span>Demo (Comming Soon)</span>
                </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="hero teaser">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--          <img src="static/images/teaser.png" alt="Teaser image."/>-->
<!--          <h2 class="subtitle has-text-justified">-->
<!--            Groma is a multimodal large language model with exceptional region understanding and visual grounding capabilities.-->
<!--            It can take user-defined region inputs (boxes) as well as generate long-form responses that are grounded to visual context.-->
<!--          </h2>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
            <p>
                We introduce NoPe-NeRF++, a novel local-to-global optimization algorithm for training Neural Radiance Fields (NeRF) without requiring pose priors.
                Specifically, compared to NoPe-NeRF, which solely explores the local relationships of images, however, fails to recover the accurate camera poses in the complex scenarios.
                To overcome the challenges, our approach begins with a relative pose initialization with explicit feature matching, followed by a local joint optimization to enhance the pose estimation for training a more robust NeRF representation.
                This method significantly improves the quality of initial poses.
                Additionally, we introduce global optimization phase that incorporates geometric consistency constraints through bundle adjustment, which integrates feature trajectories to further refine poses and collectively boost the quality of NeRF. Notably, our method is the first work that seamlessly combines the local and global cues with NeRF, and outperforms state-of-the-art methods in both pose estimation accuracy and novel view synthesis. Extensive evaluations on benchmark datasets demonstrate our superior performance and robustness, even in challenging scenes, thus validating our design choices.
            </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-four-fifths">
              <img src="static/images/teaser.jpg" alt="Teaser image."/>
              <h2 class="subtitle has-text-justified">
                Groma is a multimodal large language model with exceptional region understanding and visual grounding capabilities.
                It can take user-defined region inputs (boxes) as well as generate long-form responses that are grounded to visual context.
              </h2>
          </div>
        </div>
    </div> -->
  </div>
</section>


<br>
<br>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <a name="fig-pipline"> <img src="static/images/fig1.png" /> </a>
        <figcaption>Figure: Pipeline.</figcaption>
        <div class="content has-text-justified">
            <p>
                Given a set of images and their corresponding camera intrinsic, our method uses a local-to-global strategy to jointly estimate camera poses and the NeRF, as illustrated in <a href="#fig-pipline">figure</a>.
                To effectively handle large view changes at the start, we introduce an association relationships initialization. 
                Specifically, we construct a Maximum Spanning Tree (MST) to establish adjacency relationship among all input images, where each node represents an image, and the edge weights are determined by the maximum feature matches.
                This approach allows us to extract local matching pairs and global tracks from the tree, which proves effective in subsequent optimization.
            </p>
            <p>
                During the local joint optimization phase, we begin with the tree's root node, as it represents the image with the most matching pixels.
                We then progressively determine the relative pose between adjacent nodes based on the tree's adjacency structure to complete the initialization.
                These initial estimations serve as the starting poses for NeRF.
                Throughout training, we enforce photometric and depth consistency within a single view and geometric consistency across multiple views.
                Importantly, when introducing multi-view geometric consistency constraints, we also introduce a random matching scheme and incorporate non-adjacent random images as supervision.
                This inclusion allows global information to be integrated into local optimization, reducing accumulated errors in pose estimation and yielding more stable results.
                In the global joint optimization phase, we further refine the camera poses and NeRF by applying global geometric consistency constraints through the tracking trajectory. This involves supervising all images within the trajectory, similar to bundle adjustment, to enhance camera poses and the NeRF on a global scale.
            </p>
        </div>
      </div>
    </div>
  </div>
</section>


<br>
<br>


<!--<section class="hero teaser">
  <div class="container is-max-desktop">-->
    <!-- Abstract. -->
    <!--<div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <img src="static/images/fig2.png" />
        <figcaption>Table: Results on 7-scenes dataset.</figcaption>
        <div class="content has-text-justified">
          <p>
            First, we compare our methods with one-frame APR methods on 7-scenes dataset. The Table shows that our method achieves the best results on all indoor scenes.
            To be specific, our main pipeline, denoted as PMNet, achieves state-of-the-art performance. Moreover, PMNet<sub>ud</sub> finetuned in the self-supervising stage with unlabelled data further enhances the pose regression accuracy. The translation and rotation errors are reduced by 14.28% (0.07 &rarr; 0.06) and 12.67%(2.21 &rarr; 1.93) on average, respectively. 
            For the Heads and Pumpkin scenes, our results are comparable to those of DFNet, though not superior. Upon reviewing the dataset, we found that these scenes are relatively small, with limited visible range and camera pose variations. Consequently, the benefits of our PoseMap may not be particularly significant.
          </p>
        </div>
        <img src="static/images/fig3.png" />
        <figcaption>Table: Results on Cambridge Landmarks dataset.</figcaption>
        <div class="content has-text-justified">
          <p>
            Then we evaluate our methods on the more challenging dataset, i.e., Cambridge Landmarks. As shown in the Table, our PMNet leads to the dominant performance advantage over all scenes in single-frame APR methods. 
            Compared to DFNet<sub>dm</sub> which also uses unlabelled data for online training, our method gets a performance gain by 20.51% (0.39 &rarr; 0.31) and 17.71% (0.96 &rarr; 0.79) for translations and rotations, respectively.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>-->


<br>
<br>


<!--<section class="hero teaser">
  <div class="container is-max-desktop">-->
    <!-- Abstract. -->
    <!--<div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Visualization</h2>
        <img src="static/images/fig4.png" />
        <figcaption>Figure: Visualization.</figcaption>
        <img src="static/images/fig5.png" />
        <figcaption>Figure: 6-DOF Camera Pose Regression.</figcaption>
        <div class="content has-text-justified">
          <p>
            We visualize the camera localization sequences on 7-scenes dataset in Figure. It shows that our trajectories mostly coincide with the ground truth that means our method could estimate camera positions with high accuracy. Comparing with the SOTA method (i.e., DFNet), our method achieves better estimation results since our trajectories are much closer to the ground truth, and the number of high rotation error zones is less.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>-->

<br>
<br>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@InProceedings{lin2024posemap,
    author = {Jingyu Lin, Jiaqi Gu, Bojian Wu, Lubin Fan, Renjie Chen, Ligang Liu, Jieping Ye},
    title = {Learning Neural Volumetric Pose Features for Camera Localization},
    booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
    year = {2024}
}</code></pre>
  </div>
</section>



</body>
</html>